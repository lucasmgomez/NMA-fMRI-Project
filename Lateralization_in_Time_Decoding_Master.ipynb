{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finished Full Code"
      ],
      "metadata": {
        "id": "fH-OLgam48e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloads"
      ],
      "metadata": {
        "id": "GK_Be-L4HW99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install dependencies\n",
        "!pip install nilearn --quiet"
      ],
      "metadata": {
        "id": "KV674Zx84-Sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebc4ce1-df38-4d1f-89fe-e64e3d712288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 9.6 MB 4.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download data file\n",
        "import os, requests\n",
        "\n",
        "fname = \"hcp_task.tgz\"\n",
        "url = \"https://osf.io/2y3fw/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ],
      "metadata": {
        "id": "Y6E0MeS3Dp3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The download cells will store the data in nested directories starting here:\n",
        "HCP_DIR = \"./hcp_task\"\n",
        "\n",
        "# importing the \"tarfile\" module\n",
        "import tarfile\n",
        "\n",
        "# open file\n",
        "with tarfile.open(fname) as tfile:\n",
        "  # extracting file\n",
        "  tfile.extractall('.')\n",
        "\n",
        "subjects = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')"
      ],
      "metadata": {
        "id": "h1NVy_vSDwav",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "df83be87-111b-4b6e-b28c-00c4eaee68b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b3389c466b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msubjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHCP_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subjects_list.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
        "region_info = dict(\n",
        "    name=regions[0].tolist(),\n",
        "    network=regions[1],\n",
        "    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),\n",
        ")"
      ],
      "metadata": {
        "id": "K07rrCY-D10D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ],
      "metadata": {
        "id": "IAf0tLgzPetW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nilearn\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "_mRz7J8r5Cbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data shared for NMA projects is a subset of the full HCP dataset\n",
        "N_SUBJECTS = 100\n",
        "\n",
        "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
        "N_PARCELS = 360\n",
        "\n",
        "# The acquisition parameters for all tasks were identical\n",
        "TR = 0.72  # Time resolution, in seconds\n",
        "\n",
        "# The parcels are matched across hemispheres with the same order\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "\n",
        "# Each experiment was repeated twice in each subject\n",
        "RUNS   = ['LR','RL']\n",
        "N_RUNS = 2\n",
        "\n",
        "# There are 7 tasks. Each has a number of 'conditions'\n",
        "# TIP: look inside the data folders for more fine-graned conditions\n",
        "\n",
        "EXPERIMENTS = {\n",
        "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
        "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
        "    'EMOTION'    : {'cond':['fear','neut']},\n",
        "    'GAMBLING'   : {'cond':['loss','win']},\n",
        "    'LANGUAGE'   : {'cond':['math','story']},\n",
        "    'RELATIONAL' : {'cond':['match','relation']},\n",
        "    'SOCIAL'     : {'cond':['ment','rnd']}\n",
        "}"
      ],
      "metadata": {
        "id": "D1j3eDJk5FYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "_HF3SYNUPjLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject and single run.\n",
        "\n",
        "  Args:\n",
        "    subject (str):      subject ID to load\n",
        "    experiment (str):   Name of experiment\n",
        "    run (int):          (0 or 1)\n",
        "    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  bold_run  = RUNS[run]\n",
        "  bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
        "  bold_file = \"data.npy\"\n",
        "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
        "  if remove_mean:\n",
        "    ts -= ts.mean(axis=1, keepdims=True)\n",
        "  return ts\n",
        "\n",
        "\n",
        "def load_evs(subject, experiment, run):\n",
        "  \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
        "\n",
        "  Args:\n",
        "    subject (str): subject ID to load\n",
        "    experiment (str) : Name of experiment\n",
        "    run (int): 0 or 1\n",
        "\n",
        "  Returns\n",
        "    evs (list of lists): A list of frames associated with each condition\n",
        "\n",
        "  \"\"\"\n",
        "  frames_list = []\n",
        "  task_key = f'tfMRI_{experiment}_{RUNS[run]}'\n",
        "  for cond in EXPERIMENTS[experiment]['cond']:\n",
        "    ev_file  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
        "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
        "    ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
        "    # Determine when trial starts, rounded down\n",
        "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
        "    # Use trial duration to determine how many frames to include for trial\n",
        "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
        "    # Take the range of frames that correspond to this specific trial\n",
        "    frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
        "    frames[-1] = [i for i in frames[-1] if i < 316]\n",
        "    frames_list.append(frames)\n",
        "\n",
        "  return frames_list"
      ],
      "metadata": {
        "id": "BfP8I1bN5Lpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def block_glm_binaries(data, evs, experiment, cond):\n",
        "  \n",
        "  # Find indices/time points of conditions (e.g., math or story)\n",
        "  idx = EXPERIMENTS[experiment]['cond'].index(cond) # get the index of the condition\n",
        "  blocks = [evs[idx][i] for i in range(len(evs[idx]))] # concatenate index for condition\n",
        "  \n",
        "  # Create binarized vector of conditions\n",
        "  bins = np.zeros((data.shape[1], len(evs[idx])))\n",
        "  for b in range(0, len(blocks)):\n",
        "     bin = np.zeros(data.shape[1]) # instantiate binary vector\n",
        "     bin[blocks[b]] = 1 # make identified indices one \n",
        "     bins[:,b] = bin\n",
        "  return bins"
      ],
      "metadata": {
        "id": "9Mj6JYUo5PIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def block_tdm_binary(data, evs, experiment, cond):\n",
        "  \n",
        "  # Find indices/time points of conditions (e.g., math or story)\n",
        "  idx = EXPERIMENTS[experiment]['cond'].index(cond) # get the index of the condition\n",
        "  block = np.concatenate([evs[idx][i] for i in range(len(evs[idx]))], axis=-1) # concatenate index for condition\n",
        "  \n",
        "  # Create binarized vector of conditions\n",
        "  bin = np.zeros(data.shape[1]) # instantiate binary vector\n",
        "  bin[block] = 1 # make identified indices one \n",
        "  return bin"
      ],
      "metadata": {
        "id": "mENJM9-s5Q_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glm(Y, X, C=None, mask=None):\n",
        "  \"\"\"\n",
        "  Run a general linear model\n",
        "\n",
        "  Args:\n",
        "    Y (2d array) : time-by-space data matrix\n",
        "    X (2d array) : time-by-regressors design matrix\n",
        "    C (2d array) : contrasts-by-regressor contrrast matrix [default=Identity]\n",
        "    mask (1d array) : spatial mask wherre GLM is run\n",
        "\n",
        "  Returns:\n",
        "    contrast maps\n",
        "    t-stats\n",
        "  \"\"\"\n",
        "  if C is None:\n",
        "    C = np.identity(X.shape[1])\n",
        "  if mask is None:\n",
        "    mask = np.ones(Y.shape[1])\n",
        "\n",
        "  # initialise matrices\n",
        "  beta = np.zeros((X.shape[1], Y.shape[1]))\n",
        "  cope = np.zeros((C.shape[0], Y.shape[1]))\n",
        "  varbeta = np.zeros_like(beta)\n",
        "  tstat = np.zeros_like(beta)\n",
        "\n",
        "  # solve glm\n",
        "  beta[:, mask > 0] = np.linalg.pinv(X) @ Y[:, mask > 0]\n",
        "  # apply contrasts\n",
        "  cope[:, mask > 0] = np.dot(C, beta[:, mask > 0])\n",
        "\n",
        "  # calculate uncertainty (varcope)\n",
        "  r = Y - X@beta\n",
        "  dof = X.shape[0] - np.linalg.matrix_rank(X)\n",
        "  sig2 = np.sum(r**2, axis=0) / dof\n",
        "  varcope = np.outer(C @ np.diag(np.linalg.inv(X.T @ X)) @ C.T, sig2)\n",
        "  # calculate t-stats\n",
        "  #tstat[:, mask] = cope[:, mask] / np.sqrt(varcope[:, mask])\n",
        "\n",
        "  return cope #, tstat"
      ],
      "metadata": {
        "id": "FE6DT-fc5Uay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_glm_design_matrix(data, evs, experiment, conds):\n",
        "  \n",
        "    design_matrix = np.ones((data.shape[1], len(evs[0]) + len(evs[1]) + 1))\n",
        "\n",
        "    # print(design_matrix.shape)\n",
        "\n",
        "    idx = 1\n",
        "\n",
        "    for i in range(0,len(evs[0])+len(evs[1])):\n",
        "        if i < len(evs[0]):\n",
        "            design_matrix[:, idx] = block_glm_binaries(data, evs, experiment, conds[0])[:,i]\n",
        "        else:\n",
        "            design_matrix[:, idx] = block_glm_binaries(data, evs, experiment, conds[1])[:,i-len(evs[0])]\n",
        "        idx += 1\n",
        "    return design_matrix"
      ],
      "metadata": {
        "id": "m0P8qbpX5VNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_tdm_design_matrix(data, evs, experiment, conds):\n",
        "  design_matrix = np.ones((data.shape[1], len(conds)))\n",
        "  idx = 0\n",
        "  for cond in conds:\n",
        "    design_matrix[:, idx] = block_tdm_binary(data, evs, experiment, cond) \n",
        "    #np.convole(block_binary(data, evs, experiment, cond), HRF, 'full')[:data.shape[1]]    \n",
        "    #np.convolve(story_bin, HRF, 'full')[:data.shape[1]]\n",
        "    idx += 1\n",
        "  return design_matrix"
      ],
      "metadata": {
        "id": "C8ehhhu75Xbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_target_vec(evs):\n",
        "  target = np.zeros(len(evs[0])+len(evs[1]))\n",
        "  target[:len(evs[0])] = 1\n",
        "  return target"
      ],
      "metadata": {
        "id": "ejkWpHkM5ZSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def median_duration(evs1, evs2):\n",
        "  dur = []\n",
        "  for i in range(len(evs1)):\n",
        "    for j in range(len(evs1[i])):\n",
        "        dur.append(len(evs1[i][j]))\n",
        "  for i in range(len(evs2)):\n",
        "    for j in range(len(evs2[i])):\n",
        "        dur.append(len(evs2[i][j]))\n",
        "  med_dur = np.median(np.array(dur))\n",
        "  return np.round(med_dur)"
      ],
      "metadata": {
        "id": "fnzjp4Bw5bzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_evs(window, evs, target, bold):\n",
        "  idx = 0\n",
        "  final_targ = []\n",
        "  final_evs = [[],[]]\n",
        "  for i in range(len(evs)):\n",
        "    for j in range(len(evs[i])):\n",
        "      onset = evs[i][j][0]\n",
        "      if (onset + window) < bold.shape[1]:\n",
        "        final_targ.append(target[idx])\n",
        "        final_evs[i].append(evs[i][j])\n",
        "      idx += 1\n",
        "  final_targ = np.array(final_targ)\n",
        "  return final_targ, final_evs"
      ],
      "metadata": {
        "id": "oqEs4Fqr5fcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_window_vals(window, evs, pred):\n",
        "  val = []\n",
        "  for i in range(len(evs)):\n",
        "    for j in range(len(evs[i])):\n",
        "      onset = evs[i][j][0]\n",
        "      val.append(np.hstack((pred[onset:(onset+int(window)), 0], pred[onset:(onset+int(window)), 1])))\n",
        "  val = np.array(val)\n",
        "  return val"
      ],
      "metadata": {
        "id": "leD3YjSuULkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lateralize(data):\n",
        "    hemis = region_info['hemi']\n",
        "    l_i = hemis.index('Left')\n",
        "    \n",
        "    left = data[l_i:,:]\n",
        "    right = data[:l_i,:]\n",
        "\n",
        "    return left, right"
      ],
      "metadata": {
        "id": "RUn_7uDEW38J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run training and testing"
      ],
      "metadata": {
        "id": "RFg_w0xQPoON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_exp = 'LANGUAGE'\n",
        "\n",
        "window = []\n",
        "\n",
        "for subj in subjects:\n",
        "    # Extract timeseries\n",
        "    bold_train, bold_test = load_single_timeseries(subj, my_exp, 0, remove_mean = False), load_single_timeseries(subj, my_exp, 1, remove_mean = False)  \n",
        "\n",
        "    # Extract events\n",
        "    evs_train, evs_test = load_evs(subj, my_exp, 0), load_evs(subj, my_exp, 1)\n",
        "\n",
        "    # Generate Logistic Windows\n",
        "    window.append(median_duration(evs_train, evs_test))\n",
        "window_med = np.median(np.array(window))\n",
        "\n",
        "logit_window = np.round(window_med)"
      ],
      "metadata": {
        "id": "MRSS8pGE5hje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.linear_model\n",
        "\n",
        "\n",
        "\n",
        "#  Runs training and testing for the first n subjects (n_subjects), and returns their accuracies\n",
        "def run(n_subjects):\n",
        "    my_exp = 'LANGUAGE'\n",
        "    l_acc_glm = []\n",
        "    r_acc_glm = []\n",
        "    l_acc_tdm = []\n",
        "    r_acc_tdm = [] \n",
        "\n",
        "    # Stats dictionary\n",
        "    subject_stats = {}\n",
        "    for subj in subjects[:n_subjects]:\n",
        "        subject_stats[subj] = {}\n",
        "        subject_stats[subj]['glm'] = {}\n",
        "        subject_stats[subj]['tdm'] = {}\n",
        "\n",
        "\n",
        "        # Extract timeseries\n",
        "        bold_train, bold_test = load_single_timeseries(subj, my_exp, 0), load_single_timeseries(subj, my_exp, 1)  \n",
        "\n",
        "        # Lateralize bold data\n",
        "        l_bold_train, r_bold_train = lateralize(bold_train)\n",
        "        l_bold_test, r_bold_test = lateralize(bold_test)\n",
        "\n",
        "        # Extract events\n",
        "        evs_train, evs_test = load_evs(subj, my_exp, 0), load_evs(subj, my_exp, 1)\n",
        "        \n",
        "        # Create targets\n",
        "        target_train = gen_target_vec(evs_train)\n",
        "        target_test = gen_target_vec(evs_test)\n",
        "\n",
        "        subject_stats[subj]['target_train'] = target_train.tolist()\n",
        "\n",
        "        # Reduce EVS\n",
        "        target_train, evs_train = reduce_evs(logit_window, evs_train, target_train, bold_train)\n",
        "        target_test, evs_test = reduce_evs(logit_window, evs_test, target_test, bold_test)\n",
        "\n",
        "        ## GLM Analysis\n",
        "        \n",
        "        # Create design matrices\n",
        "        l_design_glm_train = gen_glm_design_matrix(l_bold_train, evs_train, my_exp, ['math', 'story'])\n",
        "        r_design_glm_train = gen_glm_design_matrix(r_bold_train, evs_train, my_exp, ['math', 'story'])\n",
        "        l_design_glm_test = gen_glm_design_matrix(l_bold_test, evs_test, my_exp, ['math', 'story'])\n",
        "        r_design_glm_test = gen_glm_design_matrix(r_bold_test, evs_test, my_exp, ['math', 'story'])\n",
        "\n",
        "        subject_stats[subj]['glm']['l_DMat'] = l_design_glm_train.tolist()\n",
        "        subject_stats[subj]['glm']['r_DMat'] = r_design_glm_train.tolist()\n",
        "\n",
        "        # Generate beta values\n",
        "        l_beta_train =  glm(l_bold_train.T, l_design_glm_train)\n",
        "        r_beta_train =  glm(r_bold_train.T, r_design_glm_train)\n",
        "        l_beta_test = glm(l_bold_train.T, l_design_glm_test)\n",
        "        r_beta_test = glm(r_bold_train.T, l_design_glm_test)\n",
        "\n",
        "        subject_stats[subj]['glm']['l_beta'] = l_beta_train.tolist()\n",
        "        subject_stats[subj]['glm']['r_beta'] = r_beta_train.tolist()\n",
        "\n",
        "        # Fit logistic regression with train\n",
        "        l_glm_log = sklearn.linear_model.LogisticRegressionCV(random_state=2022)\n",
        "        l_glm_log.fit(l_beta_train[1:, ], target_train)\n",
        "\n",
        "        r_glm_log = sklearn.linear_model.LogisticRegressionCV(random_state=2022)\n",
        "        r_glm_log.fit(r_beta_train[1:, ], target_train)\n",
        "        \n",
        "        # Generate predictions of model on test data\n",
        "        l_glm_pred = l_glm_log.predict(l_beta_test[1:, ])\n",
        "        r_glm_pred = r_glm_log.predict(r_beta_test[1:, ])\n",
        "\n",
        "        # Store accuracy on test data\n",
        "        l_acc = np.mean(l_glm_pred==target_test)\n",
        "        l_acc_glm.append(l_acc)\n",
        "        r_acc = np.mean(r_glm_pred==target_test)\n",
        "        r_acc_glm.append(r_acc)\n",
        "\n",
        "        subject_stats[subj]['glm']['l_acc'] = l_acc \n",
        "        subject_stats[subj]['glm']['r_acc'] = r_acc\n",
        "\n",
        "        ## Time Decoding Analysis\n",
        "\n",
        "        # Create design matrices\n",
        "        l_design_tdm_train = gen_tdm_design_matrix(l_bold_train, evs_train, my_exp, ['math', 'story'])\n",
        "        r_design_tdm_train = gen_tdm_design_matrix(r_bold_train, evs_train, my_exp, ['math', 'story'])\n",
        "        l_design_tdm_test = gen_tdm_design_matrix(l_bold_test, evs_test, my_exp, ['math', 'story'])\n",
        "        r_design_tdm_test = gen_tdm_design_matrix(r_bold_test, evs_test, my_exp, ['math', 'story'])\n",
        "\n",
        "        subject_stats[subj]['tdm']['l_DMat'] = l_design_tdm_train.tolist()\n",
        "        subject_stats[subj]['tdm']['r_DMat'] = r_design_tdm_train.tolist()\n",
        "\n",
        "        # Fit ridge regression\n",
        "        \n",
        "        # Define model\n",
        "        n_alpha = 5 # number of lambdas to test\n",
        "        alphas = np.logspace(- n_alpha / 2, n_alpha - (n_alpha / 2), num=n_alpha)\n",
        "        l_ridge = sklearn.linear_model.RidgeCV(alphas=alphas)\n",
        "        r_ridge = sklearn.linear_model.RidgeCV(alphas=alphas)\n",
        "\n",
        "        # Fit model to train data\n",
        "        l_ridge.fit(l_bold_train.T, l_design_tdm_train)\n",
        "        r_ridge.fit(r_bold_train.T, r_design_tdm_train)\n",
        "\n",
        "        subject_stats[subj]['tdm']['l_beta'] = l_ridge.coef_.tolist()\n",
        "        subject_stats[subj]['tdm']['r_beta'] = r_ridge.coef_.tolist()\n",
        "\n",
        "        # Predict stimulus train given fmri\n",
        "        l_prediction_train = l_ridge.predict(l_bold_train.T)\n",
        "        r_prediction_train = r_ridge.predict(r_bold_train.T)\n",
        "\n",
        "        # Predict stimulus test given fmri\n",
        "        l_prediction_test = l_ridge.predict(l_bold_test.T)\n",
        "        r_prediction_test = r_ridge.predict(r_bold_test.T)\n",
        "\n",
        "        # Create windows for each event\n",
        "        l_window_vals_train = gen_window_vals(logit_window, evs_train, l_prediction_train)\n",
        "        r_window_vals_train = gen_window_vals(logit_window, evs_train, r_prediction_train)\n",
        "        l_window_vals_test = gen_window_vals(logit_window, evs_test, l_prediction_test)\n",
        "        r_window_vals_test = gen_window_vals(logit_window, evs_test, r_prediction_test)\n",
        "\n",
        "        # Fit logistic regression\n",
        "        \n",
        "        # Define Model\n",
        "        l_tdm_log = sklearn.linear_model.LogisticRegressionCV(random_state=2022)\n",
        "        r_tdm_log = sklearn.linear_model.LogisticRegressionCV(random_state=2022)\n",
        "        \n",
        "        # Fit Model\n",
        "        l_tdm_log.fit(l_window_vals_train, target_train)\n",
        "        r_tdm_log.fit(r_window_vals_train, target_train)\n",
        "\n",
        "\n",
        "\n",
        "        # Generate predictions of model on test data\n",
        "        l_tdm_pred = l_tdm_log.predict(l_window_vals_test)\n",
        "        r_tdm_pred = r_tdm_log.predict(r_window_vals_test)\n",
        "\n",
        "        # Store accuracy on test data\n",
        "        l_acc = np.mean(l_tdm_pred==target_test)\n",
        "        l_acc_tdm.append(l_acc)\n",
        "        r_acc = np.mean(r_tdm_pred==target_test)\n",
        "        r_acc_tdm.append(r_acc)\n",
        "\n",
        "        subject_stats[subj]['tdm']['l_acc'] = l_acc\n",
        "        subject_stats[subj]['tdm']['r_acc'] = r_acc\n",
        "\n",
        "    return l_acc_glm, r_acc_glm, l_acc_tdm, r_acc_tdm, subject_stats\n",
        "\n",
        "a,b,c,d,_ = run(1)\n",
        "\n",
        "_['100307']['target_train']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HPbh3S4R5is-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(_['100307']['glm']['l_beta']).shape"
      ],
      "metadata": {
        "id": "bmgGRFoeU3hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bold_train, bold_test = load_single_timeseries(subj, my_exp, 0), load_single_timeseries(subj, my_exp, 1)  "
      ],
      "metadata": {
        "id": "tEF66mgyVzFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "dOzWpxo9CMqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What to do\n",
        "\n",
        "*   3D Brain map contrast of betas (language vs math)\n",
        "*   Compare accuracies between the two methods (GLM vs TDM)\n",
        "*   Compare accuracies between the two hemispheres on the language task (Left vs Right)\n",
        "\n"
      ],
      "metadata": {
        "id": "80a_UxuyCQTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example run data"
      ],
      "metadata": {
        "id": "9evUnkvQHmC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_acc_glm, r_acc_glm, l_acc_tdm, r_acc_tdm, subject_stats = run(100)"
      ],
      "metadata": {
        "id": "JxffVkCNHoQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subject_stats['100307']['target_train']"
      ],
      "metadata": {
        "id": "WMCp4Bk8ZsuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Design Matrix Plots "
      ],
      "metadata": {
        "id": "FXngEeLXSNox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(np.array(subject_stats['100307']['tdm']['l_DMat']))\n",
        "plt.legend(['math', 'story'], loc = 'upper right')\n"
      ],
      "metadata": {
        "id": "GOOjRJZISQg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Graphs"
      ],
      "metadata": {
        "id": "QeZ46r7JHqeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Left hemi\n",
        "ax.set_title('Left Hemi - Accuracies')\n",
        "plt.plot(l_acc_glm, label='glm')\n",
        "plt.plot(l_acc_tdm, label='tdm')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Right hemi\n",
        "ax.set_title('Right Hemi - Accuracies')\n",
        "plt.plot(r_acc_glm, label='glm')\n",
        "plt.plot(r_acc_tdm, label='tdm')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YHL0BpORH9s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset \n",
        "\n",
        "#l_acc_glm, r_acc_glm, l_acc_tdm, r_acc_tdm\n",
        "data_GLM_L = l_acc_glm\n",
        "data_TDM_L = l_acc_tdm\n",
        "data_GLM_R = r_acc_glm\n",
        "data_TDM_R = r_acc_tdm\n",
        "\n",
        "data = [data_GLM_L, data_TDM_L, data_GLM_R, data_TDM_R]\n",
        "\n",
        "fig = plt.figure(figsize =(8, 4))\n",
        " \n",
        "# Creating axes instance\n",
        "ax = plt.gca()\n",
        "\n",
        "#labeling x axis\n",
        "ax.set_xticklabels(['GLM_L', 'TDM_L',\n",
        "\t\t\t\t\t'GLM_R', 'TDM_R'])\n",
        " \n",
        "# Creating plot\n",
        "bp = ax.boxplot(data)\n",
        "plt.title('Accuracy Scores')\n",
        "plt.ylabel((\"Accuracy\"))\n",
        "\n",
        "#reference line\n",
        "left, right = plt.xlim()\n",
        "plt.hlines(0.5, xmin=left, xmax=right, color='r', linestyles='--')\n",
        "\n",
        "# show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZBwdCwW4dmvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Stats"
      ],
      "metadata": {
        "id": "BPvYhzFdosTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "l_tStat, l_pValue = stats.ttest_rel(l_acc_glm, l_acc_tdm)\n",
        "\n",
        "print(l_tStat, l_pValue)\n",
        "\n",
        "r_tStat, r_pValue = stats.ttest_rel(r_acc_glm, r_acc_tdm)\n",
        "\n",
        "print(r_tStat, r_pValue)\n",
        "\n",
        "lr_tStat, lr_pValue = stats.ttest_rel(l_acc_tdm, r_acc_tdm)\n",
        "\n",
        "print(lr_tStat, lr_pValue)\n"
      ],
      "metadata": {
        "id": "CD88hnjtovDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "med_l_glm = np.median(np.array(l_acc_glm))\n",
        "med_r_glm = np.median(np.array(r_acc_glm))\n",
        "print(med_l_glm, med_r_glm)\n",
        "\n",
        "med_l_tdm = np.median(np.array(l_acc_tdm))\n",
        "med_r_tdm = np.median(np.array(r_acc_tdm))\n",
        "print(med_l_tdm, med_r_tdm)"
      ],
      "metadata": {
        "id": "8wESFT_o71N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brain map"
      ],
      "metadata": {
        "id": "gXWH0cYMHog8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NMA provides an atlas. Run this cell to download it\n",
        "import os, requests\n",
        "\n",
        "# NMA provides an atlas\n",
        "fname = f\"{HCP_DIR}/atlas.npz\"\n",
        "url = \"https://osf.io/j5kuc/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)\n",
        "\n",
        "with np.load(fname) as dobj:\n",
        "  atlas = dict(**dobj)"
      ],
      "metadata": {
        "id": "TCsdquo8cGaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_betas(method='glm',hemi='l', subject_stats={}):\n",
        "    math_mean_betas = []\n",
        "    lang_mean_betas = []\n",
        "\n",
        "    for subj in subjects:\n",
        "        subj_targets = np.array(subject_stats[subj]['target_train'])\n",
        "        subj_betas = subject_stats[subj][method][hemi+'_beta']\n",
        "        num_betas = np.array(subj_betas)\n",
        "\n",
        "\n",
        "        if method == 'glm':\n",
        "            math_idxs = np.where(subj_targets == 1)[0]\n",
        "            math_betas = num_betas[math_idxs, :]\n",
        "\n",
        "            lang_idxs = np.where(subj_targets == 0)[0]\n",
        "            lang_betas = num_betas[lang_idxs, :]\n",
        "\n",
        "            mean_math_betas = np.mean(math_betas, axis = 0)      \n",
        "            mean_lang_betas = np.mean(lang_betas, axis = 0)\n",
        "            \n",
        "            math_mean_betas.append(mean_math_betas)\n",
        "            lang_mean_betas.append(mean_lang_betas)\n",
        "\n",
        "        else:\n",
        "            math_betas = num_betas[0, :]\n",
        "            lang_betas = num_betas[1, :]\n",
        "\n",
        "            math_mean_betas.append(math_betas)\n",
        "            lang_mean_betas.append(lang_betas)\n",
        "\n",
        "\n",
        "    math_avg_mean_betas = np.mean(np.array(math_mean_betas), axis = 0)\n",
        "    lang_avg_mean_betas = np.mean(np.array(lang_mean_betas), axis = 0)\n",
        "\n",
        "    return math_avg_mean_betas, lang_avg_mean_betas"
      ],
      "metadata": {
        "id": "g60pu5Wqtpbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_betas(method='glm',hemi='l', subject_stats={}):\n",
        "    math_mean_betas = []\n",
        "    lang_mean_betas = []\n",
        "\n",
        "    for subj in subjects:\n",
        "        subj_targets = np.array(subject_stats[subj]['target_train'])\n",
        "        subj_betas = subject_stats[subj][method][hemi+'_beta']\n",
        "        num_betas = np.array(subj_betas)\n",
        "\n",
        "        math_idxs = np.where(subj_targets == 1)[0]\n",
        "        math_betas = num_betas[math_idxs, :]\n",
        "        mean_math_betas = np.mean(math_betas, axis = 0)\n",
        "        \n",
        "        lang_idxs = np.where(subj_targets == 0)[0]\n",
        "        lang_betas = num_betas[lang_idxs, :]\n",
        "        mean_lang_betas = np.mean(lang_betas, axis = 0)\n",
        "\n",
        "        math_mean_betas.append(mean_math_betas)\n",
        "        lang_mean_betas.append(mean_lang_betas)\n",
        "\n",
        "    math_avg_mean_betas = np.mean(np.array(math_mean_betas), axis = 0)\n",
        "    lang_avg_mean_betas = np.mean(np.array(lang_mean_betas), axis = 0)\n",
        "\n",
        "    return math_avg_mean_betas, lang_avg_mean_betas"
      ],
      "metadata": {
        "id": "rNcOpLvPZS7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GLM Brain Plot"
      ],
      "metadata": {
        "id": "t1-uDApYdOsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_math_avg, l_lang_avg = get_mean_betas('glm','l',subject_stats)\n",
        "r_math_avg, r_lang_avg = get_mean_betas('glm','r',subject_stats)\n",
        "\n",
        "math_avg = np.concatenate([r_math_avg, l_math_avg])\n",
        "lang_avg = np.concatenate([r_lang_avg, l_lang_avg])"
      ],
      "metadata": {
        "id": "WeE24HQjdX8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9wH-AF4yKncg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_contrast = 0\n",
        "\n",
        "glm_contrast =  lang_avg - math_avg\n",
        "\n",
        "# This uses the nilearn package\n",
        "from nilearn import plotting, datasets\n",
        "\n",
        "# Try both hemispheres (L->R and left->right)\n",
        "fsaverage = datasets.fetch_surf_fsaverage()\n",
        "surf_contrast = glm_contrast[atlas[\"labels_L\"]]\n",
        "plotting.view_surf(fsaverage['infl_left'],\n",
        "                   surf_contrast,\n",
        "                   vmax=100)"
      ],
      "metadata": {
        "id": "-rZMB39LBvHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try both hemispheres (L->R and left->right)\n",
        "fsaverage = datasets.fetch_surf_fsaverage()\n",
        "surf_contrast = glm_contrast[atlas[\"labels_R\"]]\n",
        "plotting.view_surf(fsaverage['infl_right'],\n",
        "                   surf_contrast,\n",
        "                   vmax=100)"
      ],
      "metadata": {
        "id": "HnDEt44X4FCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TDM Brain Plot"
      ],
      "metadata": {
        "id": "OOZGrF4TdRDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_math_avg, l_lang_avg = get_mean_betas('tdm','l',subject_stats)\n",
        "r_math_avg, r_lang_avg = get_mean_betas('tdm','r',subject_stats)\n",
        "\n",
        "tdm_math_avg = np.concatenate([r_math_avg, l_math_avg])\n",
        "tdm_lang_avg = np.concatenate([r_lang_avg, l_lang_avg])"
      ],
      "metadata": {
        "id": "gUIJ37u0c66i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_contrast = 0\n",
        "\n",
        "tdm_contrast =  tdm_lang_avg - tdm_math_avg\n",
        "\n",
        "# This uses the nilearn package\n",
        "from nilearn import plotting, datasets\n",
        "\n",
        "# Try both hemispheres (L->R and left->right)\n",
        "fsaverage = datasets.fetch_surf_fsaverage()\n",
        "surf_contrast = tdm_contrast[atlas[\"labels_L\"]]\n",
        "plotting.view_surf(fsaverage['infl_left'],\n",
        "                   surf_contrast,\n",
        "                   vmax=0.0015)"
      ],
      "metadata": {
        "id": "qtG6Ib0qdeDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try both hemispheres (L->R and left->right)\n",
        "fsaverage = datasets.fetch_surf_fsaverage()\n",
        "surf_contrast = tdm_contrast[atlas[\"labels_R\"]]\n",
        "plotting.view_surf(fsaverage['infl_right'],\n",
        "                   surf_contrast,\n",
        "                   vmax=0.0015)"
      ],
      "metadata": {
        "id": "1gTIIr4R4qJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fu4kqz1cyPIQ",
        "h8e78_QCyPIa",
        "WmRSLa0LyPIe",
        "dvQS9IySyPIf",
        "Uy5ws8bLIYy7",
        "fLM5XoebsFTg",
        "GK_Be-L4HW99",
        "IAf0tLgzPetW",
        "_HF3SYNUPjLg",
        "RFg_w0xQPoON",
        "9evUnkvQHmC3",
        "FXngEeLXSNox"
      ],
      "provenance": []
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}